# MATH470 Final  
#### By Gavin Butts and Ava Hoeger  

In this repository, we seek to determine whether full finetuning or LoRA results in higher performances. We see that, despite LoRA's computational efficiency, full finetuning performs best. The evaluation results are provided below.  

#### Baseline
*accuracy*: 0.3382  
*precision*: 0.1144  
*recall*: 0.3382  
*f1*: 0.1710  

#### Full Fine Tuning  
*accuracy*: 0.9065  
*precision*: 0.9066   
*recall*: 0.9065  
*f1*: 0.9065  

#### LoRA
*accuracy*: 0.8815  
*precision*: 0.8814  
*recall*: 0.8815  
*f1: 0.8814 
